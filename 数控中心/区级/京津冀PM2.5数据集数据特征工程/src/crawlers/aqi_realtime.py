from src.utils.request_utils import create_session, safe_post, get_headers
from config.settings import NEWRAW_DATA_DIR, SAVE_TO_SQLITE
from src.utils.city_mapper import get_city_code_map
from src.data_processing.storage import save_raw_data, save_to_sqlite
import pandas as pd
import time
from datetime import datetime
import os
import re
import logging
import random
import requests  # ç›´æ¥å¼•å…¥requestså¤„ç†POSTè¯·æ±‚

# é…ç½®æ—¥å¿—è¾“å‡º
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('aqi_realtime_crawl.log', encoding='utf-8')  # æ–°å¢æ–‡ä»¶æ—¥å¿—
    ]
)

class AQIRealtimeCrawler:
    def __init__(self):
        self.session = create_session()
        # åŸºç¡€API URLï¼ˆé€šè¿‡citycodeåŒºåˆ†åŸå¸‚ï¼‰
        self.base_api = "https://air.cnemc.cn:18007/HourChangesPublish/GetCityRealTimeAqiHistoryByCondition"
        # åŸå¸‚ç¼–ç æ˜ å°„
        self.city_codes = get_city_code_map()
        # è®°å½•æ€»è¿›åº¦
        self.total_cities = len(self.city_codes)
        self.completed_cities = 0
        
    def _get_headers(self):
        """ç”Ÿæˆç¬¦åˆæ¥å£è¦æ±‚çš„è¯·æ±‚å¤´"""
        return get_headers(referer="https://air.cnemc.cn:18007/")  # ä¼ å…¥å®æ—¶æ¥å£çš„ Referer
    
    def _parse_timepoint(self, timepoint_str):
        """è§£æTimePointStræ ¼å¼ä¸ºæ—¥æœŸå’Œå°æ—¶ï¼ˆå¢å¼ºå®¹é”™ï¼‰"""
        if not timepoint_str:
            return None, None
            
        # åŒ¹é…æ ¼å¼å¦‚"02æ—¥20æ—¶"
        match = re.match(r"(\d{2})æ—¥(\d{2})æ—¶", timepoint_str)
        if match:
            day, hour = match.groups()
            # è·å–å½“å‰å¹´æœˆ
            now = datetime.now()
            current_year, current_month = now.year, now.month
            
            # å¤„ç†è·¨æœˆæƒ…å†µï¼ˆå¦‚æœ¬æœˆ1æ—¥çˆ¬å–ä¸Šæœˆ31æ—¥æ•°æ®ï¼‰
            try:
                datetime(current_year, current_month, int(day))
            except ValueError:
                current_month -= 1
                if current_month == 0:
                    current_month = 12
                    current_year -= 1
            
            date_str = f"{current_year}-{current_month:02d}-{day}"
            return date_str, hour
        return None, None
    
    def crawl_city_realtime(self, city_name):
        """çˆ¬å–å•ä¸ªåŸå¸‚çš„å®æ—¶AQIæ•°æ®ï¼ˆæŒ‰å°æ—¶ï¼‰"""
        logging.info(f"ğŸš€ å¼€å§‹å¤„ç†åŸå¸‚: {city_name}")
        
        city_code = self.city_codes.get(city_name)
        if not city_code:
            logging.error(f"âš ï¸ æœªæ‰¾åˆ°{city_name}çš„åŸå¸‚ç¼–ç ï¼Œè·³è¿‡è¯¥åŸå¸‚")
            return None

        # æ„é€ è¯·æ±‚å‚æ•°
        params = {
            "citycode": city_code,
            "_": int(time.time() * 1000)  # æ—¶é—´æˆ³é˜²ç¼“å­˜
        }

        # å‘é€POSTè¯·æ±‚ï¼ˆå…³é”®ä¿®æ”¹ï¼‰
        logging.info(f"ğŸš¦å‘APIå‘é€POSTè¯·æ±‚è·å–ğŸŒ {city_name}æ•°æ®...")
        response = safe_post(
            self.session,
            self.base_api,
            params=params,
            referer="https://air.cnemc.cn:18007/",  # ä¼ å…¥å®æ—¶æ¥å£çš„referer
            timeout=15
        )

        if not response:
            logging.error(f"âŒ {city_name}è¯·æ±‚å¤±è´¥ï¼Œæœªè·å–åˆ°å“åº”")
            return None

        try:
            # è§£æJSONæ•°æ®
            data = response.json()
            # å¤„ç†å¯èƒ½çš„å¤–å±‚åŒ…è£…ï¼ˆéƒ¨åˆ†æ¥å£è¿”å›æ ¼å¼å¯èƒ½åŒ…å«dataå­—æ®µï¼‰
            if isinstance(data, dict) and "data" in data:
                data = data["data"]
                
            logging.info(f"âœ… æˆåŠŸè·å–{city_name}åŸå§‹æ•°æ®ï¼Œå…±{len(data)}æ¡æ—¶é—´ç‚¹è®°å½• ğŸ“‹")
            
            all_hour_data = []
            for idx, item in enumerate(data):
                # è§£ææ—¶é—´ç‚¹
                date_str, hour = self._parse_timepoint(item.get("TimePointStr", ""))
                if not date_str or not hour:
                    logging.warning(f"â³ è·³è¿‡æ— æ•ˆæ—¶é—´æ ¼å¼è®°å½•: {item.get('TimePointStr')}")
                    continue
                    
                # ç»Ÿä¸€å¤„ç†ç©ºå€¼å’Œç‰¹æ®Šç¬¦å·
                def parse_numeric(value):
                    if value in ["â€”", "", "None", None]:
                        return None
                    try:
                        return float(value)
                    except ValueError:
                        return None

                hour_data = {
                    "åŸå¸‚": city_name,
                    "æ—¥æœŸ": date_str,
                    "å°æ—¶": hour,
                    "AQI": parse_numeric(item.get("AQI")),
                    "ç©ºæ°”è´¨é‡ç­‰çº§": item.get("Quality", "").strip(),
                    "PM2.5": parse_numeric(item.get("PM2_5")),
                    "PM10": parse_numeric(item.get("PM10")),
                    "SOâ‚‚": parse_numeric(item.get("SO2")),
                    "NOâ‚‚": parse_numeric(item.get("NO2")),
                    "CO": parse_numeric(item.get("CO")),
                    "Oâ‚ƒ": parse_numeric(item.get("O3")),
                    "é¦–è¦æ±¡æŸ“ç‰©": item.get("PrimaryPollutant", "").replace("â€”", "").strip(),
                    "å¥åº·å»ºè®®": item.get("Unheathful", "").strip(),
                    "æªæ–½å»ºè®®": item.get("Measure", "").strip(),
                    "é‡‡é›†æ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                all_hour_data.append(hour_data)
                
                # æ¯è§£æ10æ¡æ•°æ®æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if (idx + 1) % 10 == 0:
                    logging.info(f" â”œâ”€â³ {city_name}æ•°æ®è§£æè¿›åº¦: {idx + 1}/{len(data)}")

            df = pd.DataFrame(all_hour_data)
            logging.info(f" â””â”€ğŸ¯ {city_name}æ•°æ®è§£æå®Œæˆï¼Œå…±{len(df)}æ¡æœ‰æ•ˆè®°å½•")
            return df

        except Exception as e:
            logging.error(f" â””â”€ âŒ è§£æ{city_name}æ•°æ®å¤±è´¥: {str(e)}", exc_info=True)
            return None

    def crawl_realtime_batch(self, cities=None):
        """æ‰¹é‡çˆ¬å–å¤šä¸ªåŸå¸‚çš„å®æ—¶æ•°æ®"""
        cities = cities or list(self.city_codes.keys())
        all_realtime_data = []
        
        start_time = datetime.now()
        logging.info(f"=============== å¼€å§‹çˆ¬å–äº¬æ´¥å†€å®æ—¶AQIæ•°æ® ===============")
        logging.info(f"â±ï¸ çˆ¬å–æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logging.info(f"ğŸ§© å¾…çˆ¬å–åŸå¸‚æ•°é‡: {len(cities)}")
        logging.info(f"ğŸ—ºï¸ åŸå¸‚åˆ—è¡¨: {', '.join(cities)}")
        
        for i, city in enumerate(cities, 1):
            logging.info(f"\n=============== å¤„ç†ç¬¬{i}/{len(cities)}ä¸ªåŸå¸‚: {city} ===============")
            df = self.crawl_city_realtime(city)
            
            if df is not None and not df.empty:
                all_realtime_data.append(df)
                logging.info(f"âœ… {city}çˆ¬å–æˆåŠŸï¼Œè·å–{len(df)}æ¡è®°å½• ğŸ“‹")
            else:
                logging.warning(f"âŒ {city}çˆ¬å–å¤±è´¥æˆ–æ— æœ‰æ•ˆæ•°æ®")
                
            self.completed_cities = i
            # æ˜¾ç¤ºæ€»ä½“è¿›åº¦
            progress = (self.completed_cities / self.total_cities) * 100
            logging.info(f"â³ å½“å‰æ€»ä½“è¿›åº¦: {progress:.1f}% ({self.completed_cities}/{self.total_cities})")
            
            if i < len(cities):
                wait_time = random.uniform(1.5, 3.5)  # éšæœºç­‰å¾…æ—¶é—´ï¼Œé¿å…åçˆ¬
                logging.info(f"ğŸ”„ ç­‰å¾…{wait_time:.1f}ç§’åç»§ç»­ä¸‹ä¸€ä¸ªåŸå¸‚...")
                time.sleep(wait_time)
        
        if all_realtime_data:
            combined = pd.concat(all_realtime_data, ignore_index=True)
            # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
            os.makedirs(NEWRAW_DATA_DIR, exist_ok=True)
            # æŒ‰æ—¶é—´æˆ³ä¿å­˜
            filename = f"realtime_äº¬æ´¥å†€_{datetime.now().strftime('%Y%m%d_%H%M')}.csv"
            file_path = os.path.join(NEWRAW_DATA_DIR, filename)
            # ä½¿ç”¨ç»Ÿä¸€ä¿å­˜å‡½æ•°ï¼šå†™å…¥ CSVï¼ˆNEWRAW_DATA_DIR çš„ç»å¯¹è·¯å¾„ï¼‰å¹¶æ ¹æ®é…ç½®å†™å…¥ SQLite
            try:
                if SAVE_TO_SQLITE:
                    save_raw_data(combined, filename=file_path, table_name='realtime_data')
                else:
                    # ä»…ä¿å­˜ CSV
                    combined.to_csv(file_path, index=False, encoding="utf-8-sig")
                logging.info("ğŸ“„ realtime æ•°æ®ä¿å­˜å®Œæˆã€‚")
            except Exception as e:
                logging.error(f"âš ï¸ ä¿å­˜ realtime æ•°æ®å¤±è´¥ï¼š{e}")
            
            end_time = datetime.now()
            elapsed = (end_time - start_time).total_seconds()
            logging.info(f"\n=============== çˆ¬å–å®Œæˆ ===============")
            logging.info(f"â±ï¸ æ€»è€—æ—¶: {elapsed:.2f}ç§’")
            logging.info(f"ğŸ“ ä¿å­˜æ–‡ä»¶è·¯å¾„: {file_path}")
            logging.info(f"ğŸ“Œ æ€»è®°å½•æ•°: {len(combined)}æ¡")
            logging.info(f"ğŸ“ å¹³å‡æ¯ä¸ªåŸå¸‚: {len(combined)/len(cities):.1f}æ¡è®°å½•")
            logging.info(f">>>âœ… realtimeæ•°æ®çˆ¬å–ä¸ä¿å­˜æˆåŠŸï¼>>>")
            return combined
        else:
            end_time = datetime.now()
            elapsed = (end_time - start_time).total_seconds()
            logging.warning(f"\n===== çˆ¬å–å®Œæˆä½†æœªè·å–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ® =====")
            logging.info(f"â±ï¸ æ€»è€—æ—¶: {elapsed:.2f}ç§’")
            return None

if __name__ == "__main__":
    logging.info("ğŸš€ å¯åŠ¨äº¬æ´¥å†€å®æ—¶AQIæ•°æ®çˆ¬è™«...")
    try:
        crawler = AQIRealtimeCrawler()
        result = crawler.crawl_realtime_batch()
    except Exception as e:
        logging.critical(f"âŒ çˆ¬è™«è¿è¡Œå¤±è´¥: {str(e)}", exc_info=True)
    finally:
        logging.info("âœ… çˆ¬è™«æ‰§è¡Œç»“æŸ")