"""å†å²æ•°æ®çˆ¬è™«æ¨¡å—ï¼ˆaqistudy.cnï¼‰ã€‚

åŒ…å« `AQIHistoryCrawler` ç±»ï¼Œç”¨äºä½¿ç”¨ Selenium/requests è·å–ç½‘ç«™ä¸­æŒ‰æœˆ/æŒ‰åŸå¸‚çš„å†å² AQI è¡¨æ ¼æ•°æ®ï¼Œ
å¹¶å°†æ•°æ®ä»¥ CSV å½¢å¼ä¿å­˜åˆ° `data/raw`ã€‚
"""

from src.utils.request_utils import create_session, safe_get
from src.utils.selenium_utils import create_chrome_driver
from config.settings import START_YEAR, END_YEAR, REQUEST_INTERVAL, RAW_DATA_DIR
from src.utils.city_mapper import get_all_cities
from src.data_processing.storage import save_raw_data
import pandas as pd
from bs4 import BeautifulSoup
import time
import random
import os
from datetime import datetime
import re
from io import StringIO
import traceback

class AQIHistoryCrawler:
    def __init__(self):
        self.session = create_session()
        self.base_url = "https://www.aqistudy.cn/historydata/daydata.php"
        self.cities = get_all_cities()  # è·å–æ‰€æœ‰åŸå¸‚ï¼ˆä¸­æ–‡å+æ‹¼éŸ³ï¼‰
        # åˆå§‹åŒ–Seleniumé©±åŠ¨
        self.driver = create_chrome_driver()
        # é¢„è·å–æ‰€æœ‰å¯ç”¨æ—¥æœŸï¼ˆé¦–æ¬¡è¿è¡Œæ—¶æ‰§è¡Œï¼‰
        self.available_dates = self._fetch_available_dates()

    def _fetch_available_dates(self):
        """è·å–ç½‘ç«™æ”¯æŒçš„æ—¥æœŸèŒƒå›´åˆ—è¡¨"""
        sample_city = self.cities.get("ç›´è¾–å¸‚", [])[0] if self.cities else None
        if not sample_city:
            return []
        sample_url = f"{self.base_url}?city={sample_city['pinyin']}"
        dates = []
        try:
            self.driver.get(sample_url)
            time.sleep(2)  # ç­‰å¾…é¡µé¢åŠ è½½
            soup = BeautifulSoup(self.driver.page_source, 'lxml')
            dates_ = soup.find_all('li')
            for i in dates_:
                if i.a:  # å»é™¤ç©ºå€¼
                    li = i.a.text  # æå–liæ ‡ç­¾ä¸‹çš„aæ ‡ç­¾
                    date = re.findall('[0-9]*', li)  # ['2019', '', '12', '', '']
                    year = date[0]
                    month = date[2]
                    if month and year:  # å»é™¤ä¸ç¬¦åˆè¦æ±‚çš„å†…å®¹
                        date_new = '-'.join([year, month])
                        dates.append(date_new)
            # å»é‡å¹¶æ’åºï¼Œé¿å…é‡å¤çˆ¬å–
            return sorted(list(set(dates)))
        except Exception as e:
            print(f'æ—¥æœŸè·å–å¤±è´¥ï¼š{str(e)}')
            traceback.print_exc()
        return dates

    def _get_months_in_range(self, start_year, end_year):
        """ç”ŸæˆæŒ‡å®šå¹´ä»½èŒƒå›´å†…çš„æ‰€æœ‰æœˆä»½åˆ—è¡¨,æ ¼å¼YYYYMM"""
        months = []
        current_year = start_year
        current_month = 1
        end_year = end_year if datetime.now().year >= end_year else datetime.now().year
        end_month = 12 if current_year < end_year else datetime.now().month
        
        while current_year <= end_year:
            month_str = f"{current_year}{current_month:02d}"
            months.append(month_str)
            if current_year == end_year and current_month == end_month:
                break
            current_month += 1
            if current_month > 12:
                current_month = 1
                current_year += 1
        return months

    def crawl_city_month_data(self, city_pinyin, city_name, month):
        """ä½¿ç”¨Seleniumçˆ¬å–å•ä¸ªåŸå¸‚å•ä¸ªæœˆä»½çš„æ¯æ—¥AQIæ•°æ®"""
        max_retries = 5
        retry_count = 0
        while retry_count < max_retries:
            try:
                url = f"{self.base_url}?city={city_pinyin}&month={month}"
                print(f"è®¿é—®URL: {url}")
                self.driver.get(url)

                time.sleep(2 + random.uniform(0.5, 1.5))  # éšæœºç­‰å¾…æ—¶é—´
                
                # è§£æè¡¨æ ¼æ•°æ®
                html_string = StringIO(self.driver.page_source)
                tables = pd.read_html(html_string, header=0)
                
                # æ¨¡æ‹Ÿæµè§ˆå™¨æ»šåŠ¨ï¼ˆå¢å¼ºçœŸå®æ€§ï¼‰
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(random.uniform(1, 2))  # æ»šåŠ¨åç­‰å¾…
                
                if not tables or tables[0].empty:
                    print(f"{city_name}{month}æ— æœ‰æ•ˆæ•°æ®")
                    return None

                df = tables[0]
                # æ•°æ®æ¸…æ´—
                valid_cols = [col for col in ['æ—¥æœŸ', 'AQI'] if col in df.columns]
                if valid_cols:
                    df = df.dropna(subset=valid_cols)
                # è¿‡æ»¤é‡å¤è¡¨å¤´
                df = df[df['æ—¥æœŸ'] != 'æ—¥æœŸ'] if 'æ—¥æœŸ' in df.columns else df
                
                # æ·»åŠ åŸå¸‚å’Œå¹´æœˆä¿¡æ¯
                df["åŸå¸‚"] = city_name
                df["å¹´ä»½"] = month.split('-')[0]
                df["æœˆä»½"] = month.split('-')[1]
                
                # è½¬æ¢æ•°å€¼åˆ—
                numeric_cols = ["AQI", "PM2.5", "PM10", "SOâ‚‚", "NOâ‚‚", "CO", "Oâ‚ƒ", "æ’å", "O3_8h", "SO2"]
                for col in numeric_cols:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col].replace("-", None), errors="coerce")
                
                return df if not df.empty else None
                
            except Exception as e:
                retry_count += 1
                print(f"{city_name}{month}çˆ¬å–å¤±è´¥ï¼ˆç¬¬{retry_count}æ¬¡é‡è¯•ï¼‰ï¼š{str(e)}")
                # å¤±è´¥ååˆ·æ–°é¡µé¢å¹¶æ›´æ¢User-Agent
                if retry_count % 2 == 0:
                    self.driver.quit()
                    self.driver = create_chrome_driver()  # é‡å»ºé©±åŠ¨
                time.sleep(3)
        
        print(f"{city_name}{month}å¤šæ¬¡é‡è¯•å¤±è´¥ï¼Œè·³è¿‡")
        return None


    def crawl_all(self, batch_size=None):
        """çˆ¬å–æ‰€æœ‰åŸå¸‚æ‰€æœ‰æœˆä»½æ•°æ®ï¼ˆæ”¯æŒæ‰¹é‡ä¿å­˜ï¼‰"""
        batch_count = 0
        batch_data = []
        batch_size = batch_size or HISTORY_CRAWL_BATCH_SIZE  # é»˜è®¤æ¯Xä¸ªåŸå¸‚ä¿å­˜ä¸€æ¬¡
        # batch_size = batch_size or HISTORY_CRAWL_BATCH_SIZE  # é»˜è®¤æ¯X(é»˜è®¤10)ä¸ªåŸå¸‚ä¿å­˜ä¸€æ¬¡
        print(f"ä½¿ç”¨Seleniumè·å–åˆ°å¯ç”¨æ—¥æœŸ: {len(self.available_dates)}ä¸ª")
        
        # ç­›é€‰æ—¥æœŸèŒƒå›´
        filtered_dates = [
            d for d in self.available_dates 
            if START_YEAR <= int(d.split('-')[0]) <= END_YEAR
            and int(d.replace("-", "")) <= int(datetime.now().strftime("%Y%m"))
        ]
        print(f"ç­›é€‰åå¾…çˆ¬å–æœˆä»½ï¼š{filtered_dates}ï¼Œå…±{len(filtered_dates)}ä¸ª")
        
        for province, city_list in self.cities.items():
            print(f"\nå¼€å§‹çˆ¬å–{province}æ•°æ®...")
            for city in city_list:
                city_name = city["name"]
                city_pinyin = city["pinyin"]
                print(f"æ­£åœ¨çˆ¬å– {city_name}ï¼ˆ{city_pinyin}ï¼‰...")
                
                for month in filtered_dates:
                    df = self.crawl_city_month_data(city_pinyin, city_name, month)
                    if df is not None and not df.empty:
                        batch_data.append(df)
                        print(f"æˆåŠŸçˆ¬å–{city_name}{month}æ•°æ®ï¼Œå…±{len(df)}æ¡")
                    
                    # æœˆä»½é—´å¢åŠ éšæœºé—´éš”ï¼Œå¢å¼ºæŠ—åçˆ¬
                    time.sleep(REQUEST_INTERVAL + float(random.uniform(0.5, 1.5)))
                
                # åŸå¸‚é—´é—´éš”æ›´é•¿ä¸€äº›
                time.sleep(REQUEST_INTERVAL * 2 + float(random.uniform(1, 3)))
                batch_count += 1

                # æ‰¹é‡ä¿å­˜
                if batch_count >= batch_size:
                    self._save_batch(batch_data)
                    batch_data = []
                    batch_count = 0
                    # æ¯æ‰¹å®Œæˆåä¼‘æ¯ä¸€æ®µæ—¶é—´
                    time.sleep(random.uniform(5, 10))
        
        # ä¿å­˜å‰©ä½™æ•°æ®
        if batch_data:
            self._save_batch(batch_data)
        
        # çˆ¬å–å®Œæˆåå…³é—­æµè§ˆå™¨
        self.driver.quit()
        print("\nğŸ”Œ æµè§ˆå™¨å·²å…³é—­")

    def _save_batch(self, batch_data):
        """æ‰¹é‡ä¿å­˜æ•°æ®åˆ°CSV"""
        if not batch_data:
            return
        combined_df = pd.concat(batch_data, ignore_index=True)
        # æŒ‰å¹´ä»½+åŸå¸‚åˆ†ç»„ä¿å­˜ï¼ˆæ¯ä¸ªåŸå¸‚æ¯å¹´ä¸€ä¸ªæ–‡ä»¶ï¼‰
        for (year, city), df in combined_df.groupby(["å¹´ä»½", "åŸå¸‚"]):
            filename = f"{year}_{city}_aqi_history.csv"
            file_path = os.path.join(RAW_DATA_DIR, filename)
            # è¿½åŠ æ¨¡å¼ï¼Œé¿å…é‡å¤çˆ¬å–æ—¶è¦†ç›–å·²å­˜åœ¨æ•°æ®
            df.to_csv(
                file_path,
                mode='a' if os.path.exists(file_path) else 'w',
                header=not os.path.exists(file_path),
                index=False,
                encoding="utf-8-sig"
            )
        print(f"æ‰¹é‡ä¿å­˜å®Œæˆâœ…ï¼Œå…±{len(combined_df)}æ¡æ•°æ®")

if __name__ == "__main__":
    from config.settings import HISTORY_CRAWL_BATCH_SIZE
    crawler = AQIHistoryCrawler()
    crawler.crawl_all(batch_size=HISTORY_CRAWL_BATCH_SIZE)