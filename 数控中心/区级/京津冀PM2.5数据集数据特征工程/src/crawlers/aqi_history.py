"""å†å²æ•°æ®çˆ¬è™«æ¨¡å—ï¼ˆtianqihoubao.comï¼‰ã€‚

åŒ…å« `AQIHistoryCrawler` ç±»ï¼Œç”¨äºä½¿ç”¨ requests requests requests è·å–ç½‘ç«™ä¸­æŒ‰æœˆ/æŒ‰åŸå¸‚çš„å†å² AQI è¡¨æ ¼æ•°æ®ï¼Œ
å¹¶å°†æ•°æ®ä»¥ CSV å½¢å¼ä¿å­˜åˆ° `data/raw`ã€‚
"""

from src.utils.request_utils import create_session, safe_get
from config.settings import START_YEAR, END_YEAR, REQUEST_INTERVAL, RAW_DATA_DIR, HISTORY_CRAWL_BATCH_SIZE
from src.utils.city_mapper import get_all_cities
from src.data_processing.storage import save_raw_data, save_to_sqlite
from config.settings import SAVE_TO_SQLITE
import pandas as pd
from src.utils.get_ip import get_current_ip  # å¯¼å…¥IPæŸ¥è¯¢å·¥å…·
from bs4 import BeautifulSoup
import time
import random
import os
from datetime import datetime
import re
import logging
import traceback
import requests  # ç”¨äºè·å–å½“å‰IP

class AQIHistoryCrawler:
    def __init__(self):
        self.session = create_session()  # ä½¿ç”¨requestsä¼šè¯
        self.base_url = "https://www.tianqihoubao.com/aqi"
        self.cities = get_all_cities()  # è·å–æ‰€æœ‰åŸå¸‚ï¼ˆä¸­æ–‡å+æ‹¼éŸ³ï¼‰
        # ç”Ÿæˆå¯ç”¨æ—¥æœŸèŒƒå›´
        self.available_dates = self._get_months_in_range(START_YEAR, END_YEAR)
        # åˆå§‹åŒ–æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[logging.StreamHandler(), logging.FileHandler('history_crawl.log', encoding='utf-8')]
        )
        self.logger = logging.getLogger(__name__)
        # çˆ¬å–ç»Ÿè®¡å‚æ•°
        self.total_cities = sum(len(city_list) for city_list in self.cities.values())
        self.processed_cities = 0
        self.start_time = None  # æ€»è€—æ—¶è®¡æ—¶èµ·ç‚¹
        self.logger = logging.getLogger(__name__)
        # æ–°å¢ï¼šåˆå§‹åŒ–æ—¶æ£€æŸ¥ä¸€æ¬¡IP
        initial_ip = get_current_ip()
        self.logger.info(f">>>çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼ŒğŸ“ åˆå§‹IP: {initial_ip}")

    def _get_months_in_range(self, start_year, end_year):
        """ç”ŸæˆæŒ‡å®šå¹´ä»½èŒƒå›´å†…çš„æ‰€æœ‰æœˆä»½åˆ—è¡¨,æ ¼å¼YYYYMM"""
        months = []
        current_year = start_year
        current_month = 1
        end_year = end_year if datetime.now().year >= end_year else datetime.now().year
        end_month = 12 if current_year < end_year else datetime.now().month
        
        while current_year <= end_year:
            month_str = f"{current_year}{current_month:02d}"
            months.append(month_str)
            if current_year == end_year and current_month == end_month:
                break
            current_month += 1
            if current_month > 12:
                current_month = 1
                current_year += 1
        return months

    def crawl_city_month_data(self, city_pinyin, city_name, month):
        """ä½¿ç”¨requestsçˆ¬å–å•ä¸ªåŸå¸‚å•ä¸ªæœˆä»½çš„æ¯æ—¥AQIæ•°æ®"""
        max_retries = 5
        retry_count = 0
        while retry_count < max_retries:
            try:
                url = f"{self.base_url}/{city_pinyin}-{month}.html"  # æ„é€ è®¿é—®URL
                self.logger.info("=" * 60)  # åˆ†éš”çº¿
                self.logger.info(f"ğŸŒ è®¿é—®URL: {url}")
                
                # è®°å½•è¯·æ±‚å‰æ—¶é—´ï¼ˆç”¨äºå•è¯·æ±‚è€—æ—¶è®¡ç®—ï¼‰
                request_start = time.time()
                # è·å–å½“å‰IP
                current_ip = get_current_ip()
                
                # ä½¿ç”¨å®‰å…¨è¯·æ±‚æ–¹æ³•ï¼ˆå¸¦é‡è¯•å’Œéšæœºå¤´ï¼‰
                response = safe_get(
                    self.session, 
                    url, 
                    timeout=30  # å»¶é•¿è¶…æ—¶æ—¶é—´
                )
                
                # è®¡ç®—è¯·æ±‚è€—æ—¶
                request_time = time.time() - request_start
                self.logger.info(f"ğŸ“œ å½“å‰è¯·æ±‚IP >>> {current_ip}")
                if response:
                    # åªæ‰“å°å…³é”®å¤´ä¿¡æ¯
                    user_agent = response.request.headers.get("User-Agent", "æœªçŸ¥")
                    self.logger.info(f"ğŸ› ï¸  è¯·æ±‚å¤´å…³é”®ä¿¡æ¯: User-Agent={user_agent}")
                if not response:
                    self.logger.warning(f"{city_name}{month}è¯·æ±‚æ— å“åº”")
                    retry_count += 1
                    time.sleep(2 **retry_count)  # æŒ‡æ•°é€€é¿
                    continue
                self.logger.info(f"â±ï¸  è¯·æ±‚è€—æ—¶: {request_time:.2f}ç§’")
                # è§£æHTML
                soup = BeautifulSoup(response.text, 'lxml')
                table = soup.find("table", class_="b")

                if not table:
                    self.logger.warning(f"{city_name}{month}æœªæ‰¾åˆ°æ•°æ®è¡¨æ ¼")
                    return None
                
                # è§£æè¡¨å¤´ï¼ˆå¤„ç†å¯èƒ½çš„åµŒå¥—ç»“æ„ï¼‰
                headers = [th.text.strip() for th in table.find_all("tr")[0].find_all("td")]
                # è§£æè¡¨ä½“æ•°æ®
                rows = table.find_all("tr")[1:]  # è·³è¿‡è¡¨å¤´è¡Œ
                all_daily_data = []
                
                for row in rows:
                    cols = [td.text.strip() for td in row.find_all("td")]
                    if len(cols) == len(headers):
                        daily_data = dict(zip(headers, cols))
                        daily_data["åŸå¸‚"] = city_name
                        daily_data["å¹´ä»½"] = month[:4]
                        daily_data["æœˆä»½"] = month[4:]
                        all_daily_data.append(daily_data)

                if not all_daily_data:
                    self.logger.warning(f"{city_name}{month}æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®")
                    return None

                # è½¬æ¢ä¸ºDataFrameå¹¶å¤„ç†æ•°æ®ç±»å‹
                df = pd.DataFrame(all_daily_data)
                # å¤„ç†æ•°å€¼åˆ—ï¼ˆåŸæ•°æ®ä¸­çš„"-"æˆ–ç©ºå€¼è½¬æ¢ä¸ºNaNï¼‰
                numeric_cols = [
                    "AQIæŒ‡æ•°", "å½“å¤©AQIæ’å", "PM2.5", 
                    "PM10", "No2", "So2", "Co", "O3"
                ]
                for col in numeric_cols:
                    if col in df.columns:
                        df[col] = pd.to_numeric(df[col].replace("-", None), errors="coerce")
                
                self.logger.info(f"âœ… {city_name}{month}çˆ¬å–æˆåŠŸï¼Œè·å–{len(df)}æ¡è®°å½•ğŸ“Š")
                return df if not df.empty else None

            except Exception as e:
                retry_count += 1
                self.logger.error(f"âŒ {city_name}{month}çˆ¬å–å¤±è´¥ï¼ˆç¬¬{retry_count}æ¬¡é‡è¯•ï¼‰ï¼š{str(e)}")
                traceback.print_exc()
                time.sleep(2** retry_count)  # æŒ‡æ•°é€€é¿ç­‰å¾…
        
        self.logger.error(f"âŒ {city_name}{month}å¤šæ¬¡é‡è¯•å¤±è´¥ï¼Œè·³è¿‡")
        return None

    def crawl_all(self, batch_size=None):
        """çˆ¬å–æ‰€æœ‰åŸå¸‚æ‰€æœ‰æœˆä»½æ•°æ®ï¼ˆæ”¯æŒæ‰¹é‡ä¿å­˜ï¼‰"""
        batch_count = 0
        batch_data = []
        batch_size = batch_size or HISTORY_CRAWL_BATCH_SIZE  # é»˜è®¤æ¯Xä¸ªåŸå¸‚ä¿å­˜ä¸€æ¬¡
        self.start_time = time.time()  # è®°å½•æ€»å¼€å§‹æ—¶é—´
        self.logger.info(f"ğŸ“… å¯ç”¨æ—¥æœŸæ•°é‡: {len(self.available_dates)}ä¸ª")
        
        # ç­›é€‰æ—¥æœŸèŒƒå›´
        filtered_dates = [
            d for d in self.available_dates 
            if START_YEAR <= int(d[:4]) <= END_YEAR
            and int(d) <= int(datetime.now().strftime("%Y%m"))
        ]
        self.logger.info(f"ğŸ” ç­›é€‰åå¾…çˆ¬å–æœˆä»½ï¼š{filtered_dates}ï¼Œå…±{len(filtered_dates)}ä¸ª")
        self.logger.info(f"ğŸ” æ€»å¾…çˆ¬å–åŸå¸‚æ•°é‡ï¼š{self.total_cities}")
        
        for province, city_list in self.cities.items():
            self.logger.info("-" * 50)
            self.logger.info(f"ğŸš€ ã€å¼€å§‹çˆ¬å– ğŸŒ {province} æ•°æ®ã€‘")
            self.logger.info(f"ğŸ“… å¾…çˆ¬å–åŸå¸‚æ•°é‡ï¼š{len(city_list)}ä¸ª")
            self.logger.info("-" * 50)
            for idx, city in enumerate(city_list, 1):
                city_name = city["name"]
                city_pinyin = city["pinyin"]
                # åŸå¸‚çˆ¬å–æç¤º
                self.logger.info(
                    f"ğŸ” æ­£åœ¨çˆ¬å– [{province} ğŸŒ {idx}/{len(city_list)}] "
                    f"{city_name}ï¼ˆæ‹¼éŸ³ï¼š{city_pinyin}ï¼‰"
                )
                
                # è®°å½•å•ä¸ªåŸå¸‚çˆ¬å–å¼€å§‹æ—¶é—´
                city_start = time.time()
                
                for month in filtered_dates:
                    df = self.crawl_city_month_data(city_pinyin, city_name, month)
                    if df is not None and not df.empty:
                        batch_data.append(df)
                    
                    # æœˆä»½é—´å¢åŠ éšæœºé—´éš”ï¼Œå¢å¼ºæŠ—åçˆ¬
                    time.sleep(REQUEST_INTERVAL + random.uniform(0.5, 1.5))
                
                # è®¡ç®—å•ä¸ªåŸå¸‚è€—æ—¶
                city_time = time.time() - city_start
                self.processed_cities += 1
                # è®¡ç®—æ€»ä½“è¿›åº¦
                progress = (self.processed_cities / self.total_cities) * 100
                # è®¡ç®—æ€»è€—æ—¶
                total_time = time.time() - self.start_time
                
                self.logger.info("=" * 60)  # åˆ†éš”çº¿
                self.logger.info(f"ğŸ“Š ã€{city_name} å¤„ç†å®Œæˆã€‘")
                self.logger.info(f"   â”œâ”€ â±ï¸ è€—æ—¶ï¼š{city_time:.2f}ç§’")
                self.logger.info(f"   â”œâ”€ ğŸ¯ æ€»ä½“è¿›åº¦ï¼š{progress:.1f}%ï¼ˆ{self.processed_cities}/{self.total_cities}ï¼‰")
                self.logger.info(f"   â””â”€ â±ï¸ ç´¯è®¡è€—æ—¶ï¼š{total_time:.2f}ç§’")
                self.logger.info("=" * 60)  # åˆ†éš”çº¿

                
                # åŸå¸‚é—´é—´éš”æ›´é•¿ä¸€äº›
                time.sleep(REQUEST_INTERVAL * 2 + random.uniform(1, 3))
                batch_count += 1

                # æ‰¹é‡ä¿å­˜
                if batch_count >= batch_size:
                    self._save_batch(batch_data)
                    batch_data = []
                    batch_count = 0
                    # æ¯æ‰¹å®Œæˆåä¼‘æ¯ä¸€æ®µæ—¶é—´
                    time.sleep(random.uniform(5, 10))
        
        # ä¿å­˜å‰©ä½™æ•°æ®
        if batch_data:
            self._save_batch(batch_data)
            
        # æ€»è€—æ—¶ç»Ÿè®¡
        total_elapsed = time.time() - self.start_time  # è®¡ç®—æ€»è€—æ—¶
        # çˆ¬å–å®Œæˆæ—¶çš„æ±‡æ€»ä¿¡æ¯
        self.logger.info(">>>GOOD>>>")
        self.logger.info("ğŸ”Œ ã€å…¨éƒ¨çˆ¬å–ä»»åŠ¡å®Œæˆã€‘")
        self.logger.info(f"   â”œâ”€ ğŸ¯ æ€»è€—æ—¶ï¼š{total_elapsed:.2f}ç§’")
        self.logger.info(f"   â”œâ”€ ğŸŒ æ€»å¤„ç†åŸå¸‚æ•°ï¼š{self.processed_cities}/{self.total_cities}")
        self.logger.info(f"   â””â”€ ğŸ“… å®Œæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info(">>>END>>>" + "\n" + "=" * 90 + "\n")

    def _save_batch(self, batch_data):
        """æ‰¹é‡ä¿å­˜æ•°æ®åˆ°CSV"""
        if not batch_data:
            return
        combined_df = pd.concat(batch_data, ignore_index=True)
        # æŒ‰å¹´ä»½+åŸå¸‚åˆ†ç»„ä¿å­˜ï¼ˆæ¯ä¸ªåŸå¸‚æ¯å¹´ä¸€ä¸ªæ–‡ä»¶ï¼‰
        for (year, city), df in combined_df.groupby(["å¹´ä»½", "åŸå¸‚"]):
            filename = f"{year}_{city}_aqi_history.csv"
            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦åŒæ—¶å†™å…¥ SQLite
            if SAVE_TO_SQLITE:
                try:
                    save_raw_data(df, filename=filename, table_name='history_data')
                except Exception as e:
                    # å›é€€åˆ°æœ¬åœ° CSV å†™å…¥ï¼ˆå°½å¯èƒ½ä¿å­˜æ•°æ®ï¼‰
                    file_path = os.path.join(RAW_DATA_DIR, filename)
                    df.to_csv(
                        file_path,
                        mode='a' if os.path.exists(file_path) else 'w',
                        header=not os.path.exists(file_path),
                        index=False,
                        encoding="utf-8-sig"
                    )
                    self.logger.error(f"ğŸ“ ä¿å­˜åˆ° SQLite å¤±è´¥ï¼Œå·²å›é€€ä¸º CSVï¼š{e}")
            else:
                # ä»…å†™ CSV
                file_path = os.path.join(RAW_DATA_DIR, filename)
                df.to_csv(
                    file_path,
                    mode='a' if os.path.exists(file_path) else 'w',
                    header=not os.path.exists(file_path),
                    index=False,
                    encoding="utf-8-sig"
                )
        self.logger.info(f">>>ğŸ“ æ‰¹é‡ä¿å­˜å®Œæˆâœ…ï¼Œå…±{len(combined_df)}æ¡æ•°æ® ğŸ’¾")

if __name__ == "__main__":
    crawler = AQIHistoryCrawler()
    crawler.crawl_all(batch_size=HISTORY_CRAWL_BATCH_SIZE)