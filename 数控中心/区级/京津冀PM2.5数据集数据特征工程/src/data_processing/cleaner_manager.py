import os
import pandas as pd
from typing import Optional
from config.settings import RAW_DATA_DIR, NEWRAW_DATA_DIR, BASE_DIR
from src.data_processing.cleaner import clean_history as _clean_history, clean_realtime as _clean_realtime, _save_processed
from src.utils.logger import setup_logger


def run_clean_history(dir_path: str = None, merge_all: bool = True, log_file: str = None):
    """æ¸…æ´—å†å²æ•°æ®ï¼šæ‰«æ `data/Hisraw`ï¼ˆæˆ–æŒ‡å®šç›®å½•ï¼‰ä¸­çš„ CSVï¼Œé€æ–‡ä»¶è°ƒç”¨ `clean_history` å¹¶ä¿å­˜ç»“æœã€‚
    
    Args:
        dir_path: åŸå§‹æ•°æ®ç›®å½•è·¯å¾„
        merge_all: æ˜¯å¦åˆå¹¶æ‰€æœ‰æ–‡ä»¶çš„æ¸…æ´—ç»“æœä¸ºä¸€ä¸ªæ–‡ä»¶
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    try:
        from src.utils.logger import setup_logger
    except Exception as e:
        print(f"âŒ æ— æ³•å¯¼å…¥æ¸…æ´—æ¨¡å—ï¼š{e}")
        return
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger("clean_history", log_file=log_file)
    logger.info("å¼€å§‹å†å²æ•°æ®æ¸…æ´—æ“ä½œ")
    
    if dir_path is None:
        dir_path = RAW_DATA_DIR
    print(f"ğŸš€ å¼€å§‹æ¸…æ´—å†å²æ•°æ®ï¼Œç›®å½•ï¼š{dir_path}")
    logger.info(f"æ¸…æ´—å†å²æ•°æ®ï¼Œç›®å½•ï¼š{dir_path}")
    
    count = 0
    success_count = 0
    all_cleaned_data = None
    
    for fname in os.listdir(dir_path):
        logger.info(f"å‘ç°æ–‡ä»¶ï¼š{fname}")
        print(f"ğŸ” å‘ç°æ–‡ä»¶ï¼š{fname}")
        if not fname.lower().endswith('.csv'):
            logger.debug(f"è·³è¿‡éCSVæ–‡ä»¶ï¼š{fname}")
            continue
        
        fpath = os.path.join(dir_path, fname)
        try:
            df = pd.read_csv(fpath, encoding='utf-8-sig')
            logger.info(f"è¯»å–æ–‡ä»¶æˆåŠŸï¼š{fpath}ï¼Œå…± {len(df)} è¡Œæ•°æ®")
            
            cleaned_df = _clean_history(df)
            count += 1
            success_count += 1
            logger.info(f"æ¸…æ´—æ–‡ä»¶æˆåŠŸï¼š{fpath}")
            
            # æ”¶é›†æ‰€æœ‰æ¸…æ´—åçš„æ•°æ®
            if merge_all and cleaned_df is not None and not cleaned_df.empty:
                if all_cleaned_data is None:
                    all_cleaned_data = cleaned_df.copy()
                    logger.info(f"åˆå§‹åŒ–åˆå¹¶æ•°æ®é›†ï¼Œå½“å‰æ•°æ®é‡ï¼š{len(all_cleaned_data)} è¡Œ")
                else:
                    prev_count = len(all_cleaned_data)
                    all_cleaned_data = pd.concat([all_cleaned_data, cleaned_df], ignore_index=True)
                    # å»é‡
                    all_cleaned_data = all_cleaned_data.drop_duplicates(subset=[c for c in ["åŸå¸‚", "æ—¥æœŸ"] if c in all_cleaned_data.columns])
                    logger.info(f"åˆå¹¶æ•°æ®ï¼šæ–°å¢ {len(cleaned_df)} è¡Œï¼Œåˆå¹¶åå…± {len(all_cleaned_data)} è¡Œï¼Œå»é‡å‡å°‘ {prev_count + len(cleaned_df) - len(all_cleaned_data)} è¡Œ")
                    
        except Exception as e:
            logger.error(f"æ¸…æ´—æ–‡ä»¶å¤±è´¥ï¼š{fpath} -> {e}")
            print(f"âŒ æ¸…æ´—æ–‡ä»¶å¤±è´¥ï¼š{fpath} -> {e}")
    
    # ä¿å­˜åˆå¹¶åçš„ç»“æœ
    if merge_all and all_cleaned_data is not None and not all_cleaned_data.empty:
        logger.info(f"å¼€å§‹åˆå¹¶ {success_count} ä¸ªæ–‡ä»¶çš„æ¸…æ´—ç»“æœï¼Œåˆå¹¶åå…± {len(all_cleaned_data)} è¡Œæ•°æ®")
        print(f"ğŸ“Š æ­£åœ¨åˆå¹¶ {success_count} ä¸ªæ–‡ä»¶çš„æ¸…æ´—ç»“æœ...")
        
        try:
            _save_processed(all_cleaned_data, "history_merged.csv", "history_merged")
            logger.info(f"åˆå¹¶åçš„å†å²æ•°æ®å·²ä¿å­˜ï¼Œå…± {len(all_cleaned_data)} è¡Œ")
            print(f"âœ… å·²ä¿å­˜åˆå¹¶åçš„å†å²æ•°æ®")
        except Exception as e:
            logger.error(f"ä¿å­˜åˆå¹¶åçš„å†å²æ•°æ®å¤±è´¥ï¼š{e}")
            print(f"âŒ ä¿å­˜åˆå¹¶åçš„å†å²æ•°æ®å¤±è´¥ï¼š{e}")
    
    logger.info(f"å†å²æ•°æ®æ¸…æ´—å®Œæˆï¼Œå…±å¤„ç† {count} ä¸ªæ–‡ä»¶ï¼ŒæˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {count - success_count} ä¸ª")
    print(f"âœ… å†å²æ•°æ®æ¸…æ´—å®Œæˆï¼Œå…±å¤„ç†æ–‡ä»¶ï¼š{count}")


def run_clean_realtime(dir_path: str = None, merge_all: bool = True, log_file: str = None):
    """æ¸…æ´—å®æ—¶æ•°æ®ï¼šæ‰«æ `data/Newraw`ï¼ˆæˆ–æŒ‡å®šç›®å½•ï¼‰ä¸­çš„ CSVï¼Œé€æ–‡ä»¶è°ƒç”¨ `clean_realtime` å¹¶ä¿å­˜ç»“æœã€‚
    
    Args:
        dir_path: åŸå§‹æ•°æ®ç›®å½•è·¯å¾„
        merge_all: æ˜¯å¦åˆå¹¶æ‰€æœ‰æ–‡ä»¶çš„æ¸…æ´—ç»“æœä¸ºä¸€ä¸ªæ–‡ä»¶
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    try:
        from src.utils.logger import setup_logger
    except Exception as e:
        print(f"âš ï¸ æ— æ³•å¯¼å…¥æ¸…æ´—æ¨¡å—ï¼š{e}")
        return
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger("clean_realtime", log_file=log_file)
    logger.info("å¼€å§‹å®æ—¶æ•°æ®æ¸…æ´—æ“ä½œ")
    
    if dir_path is None:
        dir_path = NEWRAW_DATA_DIR
    print(f"ğŸ“‹ å¼€å§‹æ¸…æ´—å®æ—¶æ•°æ®ï¼Œç›®å½•ï¼š{dir_path}")
    logger.info(f"æ¸…æ´—å®æ—¶æ•°æ®ï¼Œç›®å½•ï¼š{dir_path}")
    
    count = 0
    success_count = 0
    all_cleaned_data = None
    
    for fname in os.listdir(dir_path):
        logger.info(f"å‘ç°æ–‡ä»¶ï¼š{fname}")
        if not fname.lower().endswith('.csv'):
            logger.debug(f"è·³è¿‡éCSVæ–‡ä»¶ï¼š{fname}")
            continue
        
        fpath = os.path.join(dir_path, fname)
        try:
            df = pd.read_csv(fpath, encoding='utf-8-sig')
            logger.info(f"è¯»å–æ–‡ä»¶æˆåŠŸï¼š{fpath}ï¼Œå…± {len(df)} è¡Œæ•°æ®")
            
            cleaned_df = _clean_realtime(df)
            count += 1
            success_count += 1
            logger.info(f"æ¸…æ´—æ–‡ä»¶æˆåŠŸï¼š{fpath}")
            
            # æ”¶é›†æ‰€æœ‰æ¸…æ´—åçš„æ•°æ®
            if merge_all and cleaned_df is not None and not cleaned_df.empty:
                if all_cleaned_data is None:
                    all_cleaned_data = cleaned_df.copy()
                    logger.info(f"åˆå§‹åŒ–åˆå¹¶æ•°æ®é›†ï¼Œå½“å‰æ•°æ®é‡ï¼š{len(all_cleaned_data)} è¡Œ")
                else:
                    prev_count = len(all_cleaned_data)
                    all_cleaned_data = pd.concat([all_cleaned_data, cleaned_df], ignore_index=True)
                    # å»é‡
                    if "ç›‘æµ‹ç«™ç‚¹" in all_cleaned_data.columns:
                        all_cleaned_data = all_cleaned_data.drop_duplicates(subset=[c for c in ["åŸå¸‚", "æ—¥æœŸ", "å°æ—¶", "ç›‘æµ‹ç«™ç‚¹"] if c in all_cleaned_data.columns])
                        logger.info(f"åˆå¹¶æ•°æ®ï¼šæ–°å¢ {len(cleaned_df)} è¡Œï¼Œåˆå¹¶åå…± {len(all_cleaned_data)} è¡Œï¼Œå»é‡å‡å°‘ {prev_count + len(cleaned_df) - len(all_cleaned_data)} è¡Œï¼ˆæŒ‰åŸå¸‚+æ—¥æœŸ+å°æ—¶+ç›‘æµ‹ç«™ç‚¹å»é‡ï¼‰")
                    else:
                        all_cleaned_data = all_cleaned_data.drop_duplicates(subset=[c for c in ["åŸå¸‚", "æ—¥æœŸ", "å°æ—¶"] if c in all_cleaned_data.columns])
                        logger.info(f"åˆå¹¶æ•°æ®ï¼šæ–°å¢ {len(cleaned_df)} è¡Œï¼Œåˆå¹¶åå…± {len(all_cleaned_data)} è¡Œï¼Œå»é‡å‡å°‘ {prev_count + len(cleaned_df) - len(all_cleaned_data)} è¡Œï¼ˆæŒ‰åŸå¸‚+æ—¥æœŸ+å°æ—¶å»é‡ï¼‰")
                    
        except Exception as e:
            logger.error(f"æ¸…æ´—æ–‡ä»¶å¤±è´¥ï¼š{fpath} -> {e}")
            print(f"âŒ æ¸…æ´—æ–‡ä»¶å¤±è´¥ï¼š{fpath} -> {e}")
    
    # ä¿å­˜åˆå¹¶åçš„ç»“æœ
    if merge_all and all_cleaned_data is not None and not all_cleaned_data.empty:
        logger.info(f"å¼€å§‹åˆå¹¶ {success_count} ä¸ªæ–‡ä»¶çš„æ¸…æ´—ç»“æœï¼Œåˆå¹¶åå…± {len(all_cleaned_data)} è¡Œæ•°æ®")
        print(f"ğŸ“Š æ­£åœ¨åˆå¹¶ {success_count} ä¸ªæ–‡ä»¶çš„æ¸…æ´—ç»“æœ...")
        
        try:
            _save_processed(all_cleaned_data, "realtime_merged.csv", "realtime_merged")
            logger.info(f"åˆå¹¶åçš„å®æ—¶æ•°æ®å·²ä¿å­˜ï¼Œå…± {len(all_cleaned_data)} è¡Œ")
            print(f"âœ… å·²ä¿å­˜åˆå¹¶åçš„å®æ—¶æ•°æ®")
        except Exception as e:
            logger.error(f"ä¿å­˜åˆå¹¶åçš„å®æ—¶æ•°æ®å¤±è´¥ï¼š{e}")
            print(f"âŒ ä¿å­˜åˆå¹¶åçš„å®æ—¶æ•°æ®å¤±è´¥ï¼š{e}")
    
    logger.info(f"å®æ—¶æ•°æ®æ¸…æ´—å®Œæˆï¼Œå…±å¤„ç† {count} ä¸ªæ–‡ä»¶ï¼ŒæˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {count - success_count} ä¸ª")
    print(f"âœ… å®æ—¶æ•°æ®æ¸…æ´—å®Œæˆï¼Œå…±å¤„ç†æ–‡ä»¶ï¼š{count}")


def run_clean():
    """åŒæ—¶æ¸…æ´—å†å²ä¸å®æ—¶æ•°æ®ï¼ˆå…ˆå†å²åå®æ—¶ï¼‰"""
    # åŒæ—¶æ¸…æ´—å†å²å’Œå®æ—¶
    try:
        
        # ä½¿ç”¨ä¸“ç”¨çš„æ¸…æ´—æ—¥å¿—æ–‡ä»¶
        clean_log_file = os.path.join(BASE_DIR, "data_processing_clean.log")
        logger = setup_logger("clean", log_file=clean_log_file)
        logger.info("å¼€å§‹æ‰§è¡Œå®Œæ•´çš„æ•°æ®æ¸…æ´—æ“ä½œ")
        logger.info("==============================================")
        
        print("ğŸ”„ å¼€å§‹æ‰§è¡Œå®Œæ•´çš„æ•°æ®æ¸…æ´—æ“ä½œ...")
        print("==============================================")
        
        # ä¼ é€’æ—¥å¿—æ–‡ä»¶å‚æ•°ç»™å­å‡½æ•°
        run_clean_history(log_file=clean_log_file)
        print("\n----------------------------------------------")
        run_clean_realtime(log_file=clean_log_file)
        
        logger.info("==============================================")
        logger.info("å®Œæ•´çš„æ•°æ®æ¸…æ´—æ“ä½œæ‰§è¡Œå®Œæˆ")
        print("\n==============================================")
        print("âœ… å®Œæ•´çš„æ•°æ®æ¸…æ´—æ“ä½œæ‰§è¡Œå®Œæˆï¼")
        print("==============================================")
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œæ¸…æ´—æ“ä½œæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
        try:
            logger.error(f"æ‰§è¡Œæ¸…æ´—æ“ä½œæ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
        except:
            pass


__all__ = ["run_clean_history", "run_clean_realtime", "run_clean"]